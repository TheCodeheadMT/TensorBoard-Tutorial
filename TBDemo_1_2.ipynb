{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "This demonstration is based off the companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). This notebook was generated for TensorFlow 2.6, but is current up to 2.12.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 22:18:53.781543: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Demo Part I - Add TensorBoard to a model \n",
    "### The IMDB dataset\n",
    "### ML problem: Binary classification, positive or negative reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loading the IMDB dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Load training and test data from imdb dataset\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000)\n",
    "\n",
    "# Values of text from this dataset integer encoded; this first sample\n",
    "# train_data[0]\n",
    "\n",
    "# The label for the first sample (1 or 0)\n",
    "# train_labels[0]\n",
    "\n",
    "# The number of trainign samples 1000 total\n",
    "# max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Decoding reviews back to text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 0s 0us/step\n",
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# If curious about the decoded text, set DECODE to True and run.\n",
    "# Only the first sample is decoded.\n",
    "DECODE = True\n",
    "if DECODE:\n",
    "        \n",
    "    word_index = imdb.get_word_index()\n",
    "    reverse_word_index = dict(\n",
    "        [(value, key) for (key, value) in word_index.items()])\n",
    "    decoded_review = \" \".join(\n",
    "        [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])\n",
    "\n",
    "    # See the decoded version of the encoded words from the imdb dataset\n",
    "    print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Encoding the integer sequences via multi-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "# create train and test datasets    \n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "# See multi-hot encoded vectorrized sequence\n",
    "# x_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Cast as floats for training\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "# Create train/validation split\n",
    "x_val = x_train[:10000]\n",
    "y_val = y_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Building your model using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb-model-1690496345 <- this will be your TensorBoard log file name.\n"
     ]
    }
   ],
   "source": [
    "# setup logging scheme\n",
    "MODEL = \"imdb-model-{}\".format(int(time.time()))\n",
    "\n",
    "# initialize callback (create logs directory first or model.fit will fail)\n",
    "tbcallback = TensorBoard(log_dir = \"logs/{}\".format(MODEL), histogram_freq=1)\n",
    "# windows?\n",
    "# tbcallback = TensorBoard(log_dir = \"logs\\{}\".format(MODEL), histogram_freq=1)\n",
    "\n",
    "print(MODEL + \" <- this will be your TensorBoard log file name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 22:19:06.821024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory:  -> device: 0, name: NVIDIA TITAN V, pci bus id: 0000:88:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-27 22:19:10.772942: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f2738cedc20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-27 22:19:10.773000: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA TITAN V, Compute Capability 7.0\n",
      "2023-07-27 22:19:10.778491: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-27 22:19:11.023264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-07-27 22:19:11.170305: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 5s 79ms/step - loss: 0.5123 - accuracy: 0.7807 - val_loss: 0.3920 - val_accuracy: 0.8559\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.3146 - accuracy: 0.8928 - val_loss: 0.3071 - val_accuracy: 0.8824\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.2373 - accuracy: 0.9186 - val_loss: 0.2829 - val_accuracy: 0.8888\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.1916 - accuracy: 0.9361 - val_loss: 0.2759 - val_accuracy: 0.8878\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.1590 - accuracy: 0.9478 - val_loss: 0.2785 - val_accuracy: 0.8880\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.1374 - accuracy: 0.9529 - val_loss: 0.2939 - val_accuracy: 0.8847\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.1177 - accuracy: 0.9633 - val_loss: 0.3035 - val_accuracy: 0.8849\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.1012 - accuracy: 0.9693 - val_loss: 0.3174 - val_accuracy: 0.8826\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0862 - accuracy: 0.9751 - val_loss: 0.3354 - val_accuracy: 0.8806\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0733 - accuracy: 0.9805 - val_loss: 0.3518 - val_accuracy: 0.8819\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0650 - accuracy: 0.9820 - val_loss: 0.3742 - val_accuracy: 0.8760\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.0548 - accuracy: 0.9853 - val_loss: 0.3914 - val_accuracy: 0.8768\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0480 - accuracy: 0.9878 - val_loss: 0.4161 - val_accuracy: 0.8760\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.0396 - accuracy: 0.9908 - val_loss: 0.4403 - val_accuracy: 0.8742\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.0341 - accuracy: 0.9926 - val_loss: 0.5380 - val_accuracy: 0.8607\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0328 - accuracy: 0.9922 - val_loss: 0.4838 - val_accuracy: 0.8718\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0273 - accuracy: 0.9943 - val_loss: 0.5048 - val_accuracy: 0.8714\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0204 - accuracy: 0.9978 - val_loss: 0.5277 - val_accuracy: 0.8698\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0195 - accuracy: 0.9969 - val_loss: 0.5474 - val_accuracy: 0.8692\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.0167 - accuracy: 0.9975 - val_loss: 0.5727 - val_accuracy: 0.8690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2dfd480fa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a sequential fully connected network to address the ML problem\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\", name=\"L1_16_Dense_RELU\"),\n",
    "    layers.Dense(16, activation=\"relu\", name=\"L2_16_Dense_RELU\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\", name=\"L3_1_Dense_SIG\")\n",
    "])\n",
    "\n",
    "# Compile the network using your optmizier, loss, and metrics\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit your model, including TensorBoard in the callback list\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train, \n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=20, \n",
    "          batch_size=512, \n",
    "          callbacks=[tbcallback])\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review TensorBoard output to evaluate how model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In console, from directory containing the \"logs\" directory.\n",
    "# tensorboard --logdir logs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refit model with adjusted parameters and all training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "49/49 [==============================] - 3s 18ms/step - loss: 0.4601 - accuracy: 0.0000e+00 - prc: 0.8953\n",
      "Epoch 2/5\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.2694 - accuracy: 4.0000e-05 - prc: 0.9637\n",
      "Epoch 3/5\n",
      "49/49 [==============================] - 1s 16ms/step - loss: 0.2112 - accuracy: 4.0000e-05 - prc: 0.9759\n",
      "Epoch 4/5\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.1796 - accuracy: 4.4000e-04 - prc: 0.9818\n",
      "Epoch 5/5\n",
      "49/49 [==============================] - 1s 12ms/step - loss: 0.1581 - accuracy: 0.0012 - prc: 0.9855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2dfe070e80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make any adjustments for your final model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# added in a Precision Recall curve metric\n",
    "metrics=[tf.keras.metrics.Accuracy(name='accuracy'),\n",
    "        tf.keras.metrics.AUC(name='prc', curve='PR')]\n",
    "\n",
    "\n",
    "# Compile the network using your optmizier, loss, and metrics\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=metrics)\n",
    "\n",
    "# Update logging scheme\n",
    "MODEL = \"imdb-final-model-{}\".format(int(time.time()))\n",
    "tbcallback = TensorBoard(log_dir = \"logs/{}\".format(MODEL), histogram_freq=1)\n",
    "# tbcallback = TensorBoard(log_dir = \"logs\\{}\".format(MODEL), histogram_freq=1)\n",
    "\n",
    "# Refit your final model\n",
    "model.fit(x_train,                              # No longer partial_x_train\n",
    "          y_train,                              # No longer partial_y_train\n",
    "          epochs=5,                             # Changed to 5 epochs \n",
    "          batch_size=512, \n",
    "          callbacks=[tbcallback])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2975 - accuracy: 4.0000e-04 - prc: 0.9459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.29754045605659485, 0.00039999998989515007, 0.9458764791488647]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check how accurate the model is at predicting unseen observations\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "# A tuple containing loss, accuracy\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Using a trained model to generate predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.18177032],\n",
       "       [0.9999422 ],\n",
       "       [0.8168049 ],\n",
       "       ...,\n",
       "       [0.0925273 ],\n",
       "       [0.04993724],\n",
       "       [0.5770854 ]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Part 2: Comparing multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Classifying newswires: A multiclass classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The Reuters dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loading the Reuters dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2110848/2110848 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
    "    num_words=10000)\n",
    "\n",
    "#len(train_data)\n",
    "#len(test_data)\n",
    "#train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Decoding newswires back to text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
      "550378/550378 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in\n",
    "    train_data[0]])\n",
    "\n",
    "#train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Encoding the input and labels & Splitting data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "\n",
    "#Encode input\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "#Encode labels\n",
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "y_train = to_one_hot(train_labels)\n",
    "y_test = to_one_hot(test_labels)\n",
    "\n",
    "# import 1-hot encoding library\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(train_labels)\n",
    "y_test = to_categorical(test_labels)\n",
    "\n",
    "#Set aside validation data\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = y_train[:1000]\n",
    "partial_y_train = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Building your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create two models, the only difference is hidden layer activations\n",
    "\n",
    "model1 = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"tanh\"),\n",
    "    layers.Dense(64, activation=\"tanh\"),\n",
    "    layers.Dense(46, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model2 = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(46, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Compiling the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model2.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 3s 62ms/step - loss: 2.4063 - accuracy: 0.5263 - val_loss: 1.6900 - val_accuracy: 0.6220\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 1.4525 - accuracy: 0.6840 - val_loss: 1.3365 - val_accuracy: 0.6970\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 1.1204 - accuracy: 0.7635 - val_loss: 1.1352 - val_accuracy: 0.7650\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.8966 - accuracy: 0.8260 - val_loss: 1.0182 - val_accuracy: 0.7860\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.7295 - accuracy: 0.8623 - val_loss: 0.9301 - val_accuracy: 0.8070\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.5992 - accuracy: 0.8860 - val_loss: 0.8783 - val_accuracy: 0.8190\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.4968 - accuracy: 0.9064 - val_loss: 0.8469 - val_accuracy: 0.8220\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.4094 - accuracy: 0.9242 - val_loss: 0.8248 - val_accuracy: 0.8250\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 0.3417 - accuracy: 0.9369 - val_loss: 0.8252 - val_accuracy: 0.8250\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.2902 - accuracy: 0.9419 - val_loss: 0.8289 - val_accuracy: 0.8200\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.2461 - accuracy: 0.9464 - val_loss: 0.8228 - val_accuracy: 0.8160\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.2127 - accuracy: 0.9513 - val_loss: 0.8249 - val_accuracy: 0.8220\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.1869 - accuracy: 0.9515 - val_loss: 0.8265 - val_accuracy: 0.8180\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.1657 - accuracy: 0.9560 - val_loss: 0.8494 - val_accuracy: 0.8120\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.1515 - accuracy: 0.9558 - val_loss: 0.8416 - val_accuracy: 0.8150\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.1398 - accuracy: 0.9543 - val_loss: 0.8627 - val_accuracy: 0.8070\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.1271 - accuracy: 0.9563 - val_loss: 0.8660 - val_accuracy: 0.8040\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 0.1206 - accuracy: 0.9572 - val_loss: 0.9059 - val_accuracy: 0.8030\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.1149 - accuracy: 0.9559 - val_loss: 0.8980 - val_accuracy: 0.8060\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.1077 - accuracy: 0.9563 - val_loss: 0.9004 - val_accuracy: 0.8030\n",
      "Epoch 1/20\n",
      "16/16 [==============================] - 3s 57ms/step - loss: 2.7046 - accuracy: 0.4068 - val_loss: 1.8330 - val_accuracy: 0.6280\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 1.5204 - accuracy: 0.6883 - val_loss: 1.3605 - val_accuracy: 0.6990\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 1.1501 - accuracy: 0.7507 - val_loss: 1.1729 - val_accuracy: 0.7440\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.9346 - accuracy: 0.7994 - val_loss: 1.0722 - val_accuracy: 0.7620\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.7648 - accuracy: 0.8396 - val_loss: 0.9732 - val_accuracy: 0.7920\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.6315 - accuracy: 0.8682 - val_loss: 0.9223 - val_accuracy: 0.8020\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.5284 - accuracy: 0.8913 - val_loss: 0.9015 - val_accuracy: 0.8070\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.4372 - accuracy: 0.9103 - val_loss: 0.8841 - val_accuracy: 0.8060\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.3713 - accuracy: 0.9236 - val_loss: 0.8884 - val_accuracy: 0.8080\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.3210 - accuracy: 0.9322 - val_loss: 0.8885 - val_accuracy: 0.8060\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.2772 - accuracy: 0.9409 - val_loss: 0.8452 - val_accuracy: 0.8290\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.2422 - accuracy: 0.9445 - val_loss: 0.8749 - val_accuracy: 0.8160\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.2182 - accuracy: 0.9481 - val_loss: 0.8628 - val_accuracy: 0.8250\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.1973 - accuracy: 0.9528 - val_loss: 0.8690 - val_accuracy: 0.8230\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.1749 - accuracy: 0.9531 - val_loss: 0.8667 - val_accuracy: 0.8330\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 0.1667 - accuracy: 0.9551 - val_loss: 0.8951 - val_accuracy: 0.8190\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 1s 44ms/step - loss: 0.1549 - accuracy: 0.9555 - val_loss: 0.8983 - val_accuracy: 0.8180\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.1465 - accuracy: 0.9554 - val_loss: 0.9105 - val_accuracy: 0.8190\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.1440 - accuracy: 0.9555 - val_loss: 0.9274 - val_accuracy: 0.8210\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.1298 - accuracy: 0.9573 - val_loss: 0.9214 - val_accuracy: 0.8160\n"
     ]
    }
   ],
   "source": [
    "# setup logging scheme for model1\n",
    "MODEL = \"mod1-reuters-{}\".format(int(time.time()))\n",
    "tbcallback = TensorBoard(log_dir = \"logs/{}\".format(MODEL), histogram_freq=1)\n",
    "#tbcallback = TensorBoard(log_dir = \"logs\\{}\".format(MODEL), histogram_freq=1)\n",
    "\n",
    "history1 = model1.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[tbcallback])\n",
    "\n",
    "MODEL = \"mod2-reuters-{}\".format(int(time.time()))\n",
    "tbcallback = TensorBoard(log_dir = \"logs/{}\".format(MODEL), histogram_freq=1)\n",
    "#tbcallback = TensorBoard(log_dir = \"logs\\{}\".format(MODEL), histogram_freq=1)\n",
    "\n",
    "history2 = model2.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[tbcallback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Retraining a model from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/18 [==============================] - 2s 37ms/step - loss: 2.6620 - accuracy: 0.5163\n",
      "Epoch 2/20\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 1.4816 - accuracy: 0.6820\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 1.1367 - accuracy: 0.7509\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.9262 - accuracy: 0.7976\n",
      "Epoch 5/20\n",
      "18/18 [==============================] - 1s 39ms/step - loss: 0.7662 - accuracy: 0.8340\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.6389 - accuracy: 0.8588\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.5301 - accuracy: 0.8828\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.4493 - accuracy: 0.9019\n",
      "Epoch 9/20\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.3755 - accuracy: 0.9176\n",
      "Epoch 10/20\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.3189 - accuracy: 0.9282\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.2799 - accuracy: 0.9359\n",
      "Epoch 12/20\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 0.2475 - accuracy: 0.9432\n",
      "Epoch 13/20\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.2158 - accuracy: 0.9467\n",
      "Epoch 14/20\n",
      "18/18 [==============================] - 1s 34ms/step - loss: 0.1987 - accuracy: 0.9479\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.1812 - accuracy: 0.9510\n",
      "Epoch 16/20\n",
      "18/18 [==============================] - 1s 36ms/step - loss: 0.1680 - accuracy: 0.9531\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - 1s 32ms/step - loss: 0.1581 - accuracy: 0.9541\n",
      "Epoch 18/20\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.1479 - accuracy: 0.9542\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 0.1433 - accuracy: 0.9537\n",
      "Epoch 20/20\n",
      "18/18 [==============================] - 1s 35ms/step - loss: 0.1355 - accuracy: 0.9555\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 1.0026 - accuracy: 0.8014\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"final-reuters-{}\".format(int(time.time()))\n",
    "tbcallback = TensorBoard(log_dir = \"logs/{}\".format(MODEL), histogram_freq=1)\n",
    "#tbcallback = TensorBoard(log_dir = \"logs\\{}\".format(MODEL), histogram_freq=1)\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(64, activation=\"relu\"),\n",
    "  layers.Dense(46, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=20,\n",
    "          batch_size=512,\n",
    "          callbacks=[tbcallback])\n",
    "          \n",
    "results = model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0026326179504395, 0.8014247417449951]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17764915405164738"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "hits_array = np.array(test_labels) == np.array(test_labels_copy)\n",
    "hits_array.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Generating predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99999994"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter04_getting-started-with-neural-networks.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
