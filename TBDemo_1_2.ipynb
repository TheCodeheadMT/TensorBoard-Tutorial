{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "This demonstration is based off the companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). This notebook was generated for TensorFlow 2.6, but is current up to 2.12.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 20:28:47.341424: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-31 20:28:47.391896: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Demo Part I - Add TensorBoard to a model \n",
    "### The IMDB dataset\n",
    "### ML problem: Binary classification, positive or negative reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loading the IMDB dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Load training and test data from imdb dataset\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000)\n",
    "\n",
    "# Values of text from this dataset integer encoded; this first sample\n",
    "# train_data[0]\n",
    "\n",
    "# The label for the first sample (1 or 0)\n",
    "# train_labels[0]\n",
    "\n",
    "# The number of trainign samples 1000 total\n",
    "# max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Decoding reviews back to text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# If curious about the decoded text, set DECODE to True and run.\n",
    "# Only the first sample is decoded.\n",
    "DECODE = True\n",
    "if DECODE:\n",
    "        \n",
    "    word_index = imdb.get_word_index()\n",
    "    reverse_word_index = dict(\n",
    "        [(value, key) for (key, value) in word_index.items()])\n",
    "    decoded_review = \" \".join(\n",
    "        [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])\n",
    "\n",
    "    # See the decoded version of the encoded words from the imdb dataset\n",
    "    print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Encoding the integer sequences via multi-hot encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "# create train and test datasets    \n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "# See multi-hot encoded vectorrized sequence\n",
    "# x_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Cast as floats for training\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "# Create train/validation split\n",
    "x_val = x_train[:10000]\n",
    "y_val = y_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Building your model using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb-model-1690835335 <- this will be your TensorBoard log file name.\n"
     ]
    }
   ],
   "source": [
    "# setup logging scheme\n",
    "MODEL = \"imdb-model-{}\".format(int(time.time()))\n",
    "\n",
    "# initialize callback (create logs directory first or model.fit will fail)\n",
    "tbcallback = TensorBoard(log_dir = \"logs/{}\".format(MODEL), histogram_freq=1)\n",
    "# windows?\n",
    "# tbcallback = TensorBoard(log_dir = \"logs\\{}\".format(MODEL), histogram_freq=1)\n",
    "\n",
    "print(MODEL + \" <- this will be your TensorBoard log file name.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 20:28:56.492025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30925 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 20:29:02.107471: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fbd10017a80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-31 20:29:02.107546: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2023-07-31 20:29:02.117429: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-31 20:29:02.331520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2023-07-31 20:29:02.489866: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 7s 66ms/step - loss: 0.5591 - accuracy: 0.7641 - val_loss: 0.4371 - val_accuracy: 0.8529\n",
      "Epoch 2/20\n",
      "30/30 [==============================] - 1s 22ms/step - loss: 0.3616 - accuracy: 0.8878 - val_loss: 0.3506 - val_accuracy: 0.8715\n",
      "Epoch 3/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.2721 - accuracy: 0.9131 - val_loss: 0.2955 - val_accuracy: 0.8853\n",
      "Epoch 4/20\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.2187 - accuracy: 0.9290 - val_loss: 0.2917 - val_accuracy: 0.8843\n",
      "Epoch 5/20\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.1804 - accuracy: 0.9420 - val_loss: 0.2819 - val_accuracy: 0.8826\n",
      "Epoch 6/20\n",
      "30/30 [==============================] - 1s 21ms/step - loss: 0.1546 - accuracy: 0.9503 - val_loss: 0.2832 - val_accuracy: 0.8843\n",
      "Epoch 7/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1330 - accuracy: 0.9568 - val_loss: 0.3358 - val_accuracy: 0.8703\n",
      "Epoch 8/20\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.1157 - accuracy: 0.9648 - val_loss: 0.2968 - val_accuracy: 0.8846\n",
      "Epoch 9/20\n",
      "30/30 [==============================] - 1s 25ms/step - loss: 0.1023 - accuracy: 0.9689 - val_loss: 0.3163 - val_accuracy: 0.8828\n",
      "Epoch 10/20\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.0880 - accuracy: 0.9743 - val_loss: 0.3373 - val_accuracy: 0.8748\n",
      "Epoch 11/20\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.0775 - accuracy: 0.9784 - val_loss: 0.3588 - val_accuracy: 0.8796\n",
      "Epoch 12/20\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.0684 - accuracy: 0.9819 - val_loss: 0.3617 - val_accuracy: 0.8808\n",
      "Epoch 13/20\n",
      "30/30 [==============================] - 1s 23ms/step - loss: 0.0580 - accuracy: 0.9859 - val_loss: 0.3786 - val_accuracy: 0.8768\n",
      "Epoch 14/20\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.0478 - accuracy: 0.9897 - val_loss: 0.4009 - val_accuracy: 0.8783\n",
      "Epoch 15/20\n",
      "30/30 [==============================] - 1s 24ms/step - loss: 0.0426 - accuracy: 0.9915 - val_loss: 0.4259 - val_accuracy: 0.8714\n",
      "Epoch 16/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0372 - accuracy: 0.9925 - val_loss: 0.4476 - val_accuracy: 0.8746\n",
      "Epoch 17/20\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.0318 - accuracy: 0.9940 - val_loss: 0.4771 - val_accuracy: 0.8743\n",
      "Epoch 18/20\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.0264 - accuracy: 0.9955 - val_loss: 0.5064 - val_accuracy: 0.8720\n",
      "Epoch 19/20\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0237 - accuracy: 0.9961 - val_loss: 0.5100 - val_accuracy: 0.8708\n",
      "Epoch 20/20\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0203 - accuracy: 0.9971 - val_loss: 0.5313 - val_accuracy: 0.8709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc17cf1b820>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize a sequential fully connected network to address the ML problem\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\", name=\"L1_16_Dense_RELU\"),\n",
    "    layers.Dense(16, activation=\"relu\", name=\"L2_16_Dense_RELU\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\", name=\"L3_1_Dense_SIG\")\n",
    "])\n",
    "\n",
    "# Compile the network using your optmizier, loss, and metrics\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit your model, including TensorBoard in the callback list\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train, \n",
    "          validation_data=(x_val, y_val),\n",
    "          epochs=20, \n",
    "          batch_size=512, \n",
    "          callbacks=[tbcallback])\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review TensorBoard output to evaluate how model performance.\n",
    "#### STOP and run all cells above and return to slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In console, from directory containing the \"logs\" directory.\n",
    "# tensorboard --logdir logs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refit model with adjusted parameters and all training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "49/49 [==============================] - 5s 25ms/step - loss: 0.4717 - accuracy: 0.0000e+00 - prc: 0.8974\n",
      "Epoch 2/5\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.2828 - accuracy: 0.0000e+00 - prc: 0.9602\n",
      "Epoch 3/5\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.2196 - accuracy: 4.0000e-05 - prc: 0.9731\n",
      "Epoch 4/5\n",
      "49/49 [==============================] - 1s 14ms/step - loss: 0.1868 - accuracy: 4.0000e-05 - prc: 0.9799\n",
      "Epoch 5/5\n",
      "49/49 [==============================] - 1s 15ms/step - loss: 0.1643 - accuracy: 4.0000e-05 - prc: 0.9841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc17eb8ddc0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make any adjustments for your final model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# added in a Precision Recall curve metric\n",
    "metrics=[tf.keras.metrics.Accuracy(name='accuracy'),\n",
    "        tf.keras.metrics.AUC(name='prc', curve='PR')]\n",
    "\n",
    "\n",
    "# Compile the network using your optmizier, loss, and metrics\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=metrics)\n",
    "\n",
    "# Update logging scheme\n",
    "MODEL = \"imdb-final-model-{}\".format(int(time.time()))\n",
    "tbcallback = TensorBoard(log_dir = \"logs/{}\".format(MODEL), histogram_freq=1)\n",
    "# tbcallback = TensorBoard(log_dir = \"logs\\{}\".format(MODEL), histogram_freq=1)\n",
    "\n",
    "# Refit your final model\n",
    "model.fit(x_train,                              # No longer partial_x_train\n",
    "          y_train,                              # No longer partial_y_train\n",
    "          epochs=5,                             # Changed to 5 epochs \n",
    "          batch_size=512, \n",
    "          callbacks=[tbcallback])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 4ms/step - loss: 0.2930 - accuracy: 4.0000e-05 - prc: 0.9443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2929645776748657, 3.9999998989515007e-05, 0.9442622661590576]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Check how accurate the model is at predicting unseen observations\n",
    "results = model.evaluate(x_test, y_test)\n",
    "\n",
    "# A tuple containing loss, accuracy\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Using a trained model to generate predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.17112797],\n",
       "       [0.9997923 ],\n",
       "       [0.81388503],\n",
       "       ...,\n",
       "       [0.10509251],\n",
       "       [0.06741008],\n",
       "       [0.68772835]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Part 2: Comparing multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Classifying newswires: A multiclass classification example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The Reuters dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.datasets import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Loading the Reuters dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
    "    num_words=10000)\n",
    "\n",
    "#len(train_data)\n",
    "#len(test_data)\n",
    "#train_data[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Decoding newswires back to text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in\n",
    "    train_data[0]])\n",
    "\n",
    "#train_labels[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Encoding the input and labels & Splitting data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "\n",
    "#Encode input\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "#Encode labels\n",
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] = 1.\n",
    "    return results\n",
    "y_train = to_one_hot(train_labels)\n",
    "y_test = to_one_hot(test_labels)\n",
    "\n",
    "# import 1-hot encoding library\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(train_labels)\n",
    "y_test = to_categorical(test_labels)\n",
    "\n",
    "#Set aside validation data\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = y_train[:1000]\n",
    "partial_y_train = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Building your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Create two models, the only difference is hidden layer activations\n",
    "\n",
    "model1 = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"tanh\"),\n",
    "    layers.Dense(64, activation=\"tanh\"),\n",
    "    layers.Dense(46, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model2 = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(46, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Compiling the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model1.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model2.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16/16 [==============================] - 6s 80ms/step - loss: 2.4180 - accuracy: 0.4872 - val_loss: 1.7011 - val_accuracy: 0.6300\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 1.4533 - accuracy: 0.6849 - val_loss: 1.3085 - val_accuracy: 0.7040\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 1.1158 - accuracy: 0.7711 - val_loss: 1.1186 - val_accuracy: 0.7620\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 1s 48ms/step - loss: 0.8955 - accuracy: 0.8202 - val_loss: 1.0007 - val_accuracy: 0.7960\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.7320 - accuracy: 0.8544 - val_loss: 0.9253 - val_accuracy: 0.8020\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.6015 - accuracy: 0.8857 - val_loss: 0.8696 - val_accuracy: 0.8230\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 1s 42ms/step - loss: 0.4984 - accuracy: 0.9062 - val_loss: 0.8357 - val_accuracy: 0.8270\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 1s 43ms/step - loss: 0.4106 - accuracy: 0.9243 - val_loss: 0.8097 - val_accuracy: 0.8270\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.3431 - accuracy: 0.9347 - val_loss: 0.8082 - val_accuracy: 0.8180\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.2874 - accuracy: 0.9431 - val_loss: 0.8218 - val_accuracy: 0.8170\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.2456 - accuracy: 0.9468 - val_loss: 0.8048 - val_accuracy: 0.8240\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.2128 - accuracy: 0.9513 - val_loss: 0.8091 - val_accuracy: 0.8250\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.1888 - accuracy: 0.9513 - val_loss: 0.8241 - val_accuracy: 0.8170\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 0.1677 - accuracy: 0.9529 - val_loss: 0.8365 - val_accuracy: 0.8230\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.1528 - accuracy: 0.9545 - val_loss: 0.8317 - val_accuracy: 0.8200\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 1s 40ms/step - loss: 0.1402 - accuracy: 0.9564 - val_loss: 0.8545 - val_accuracy: 0.8140\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 1s 49ms/step - loss: 0.1289 - accuracy: 0.9557 - val_loss: 0.8705 - val_accuracy: 0.8160\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.1223 - accuracy: 0.9568 - val_loss: 0.8602 - val_accuracy: 0.8200\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.1165 - accuracy: 0.9557 - val_loss: 0.8755 - val_accuracy: 0.8130\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 1s 45ms/step - loss: 0.1102 - accuracy: 0.9560 - val_loss: 0.9017 - val_accuracy: 0.8070\n",
      "Epoch 1/20\n",
      "16/16 [==============================] - 2s 57ms/step - loss: 2.6768 - accuracy: 0.5147 - val_loss: 1.8383 - val_accuracy: 0.6290\n",
      "Epoch 2/20\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 1.5522 - accuracy: 0.6751 - val_loss: 1.3971 - val_accuracy: 0.6880\n",
      "Epoch 3/20\n",
      "16/16 [==============================] - 0s 30ms/step - loss: 1.2050 - accuracy: 0.7339 - val_loss: 1.2043 - val_accuracy: 0.7340\n",
      "Epoch 4/20\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.9965 - accuracy: 0.7825 - val_loss: 1.0992 - val_accuracy: 0.7610\n",
      "Epoch 5/20\n",
      "16/16 [==============================] - 0s 28ms/step - loss: 0.8293 - accuracy: 0.8235 - val_loss: 1.0209 - val_accuracy: 0.7800\n",
      "Epoch 6/20\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.6953 - accuracy: 0.8512 - val_loss: 0.9662 - val_accuracy: 0.7910\n",
      "Epoch 7/20\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.5795 - accuracy: 0.8797 - val_loss: 0.9171 - val_accuracy: 0.8120\n",
      "Epoch 8/20\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.4891 - accuracy: 0.8970 - val_loss: 0.8860 - val_accuracy: 0.8160\n",
      "Epoch 9/20\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.4161 - accuracy: 0.9094 - val_loss: 0.9358 - val_accuracy: 0.7930\n",
      "Epoch 10/20\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 0.3525 - accuracy: 0.9251 - val_loss: 0.8662 - val_accuracy: 0.8160\n",
      "Epoch 11/20\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.3088 - accuracy: 0.9321 - val_loss: 0.8763 - val_accuracy: 0.8140\n",
      "Epoch 12/20\n",
      "16/16 [==============================] - 1s 34ms/step - loss: 0.2707 - accuracy: 0.9394 - val_loss: 0.8506 - val_accuracy: 0.8270\n",
      "Epoch 13/20\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.2331 - accuracy: 0.9466 - val_loss: 0.8840 - val_accuracy: 0.8270\n",
      "Epoch 14/20\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.2099 - accuracy: 0.9481 - val_loss: 0.9275 - val_accuracy: 0.8050\n",
      "Epoch 15/20\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.1958 - accuracy: 0.9506 - val_loss: 0.9041 - val_accuracy: 0.8150\n",
      "Epoch 16/20\n",
      "16/16 [==============================] - 1s 37ms/step - loss: 0.1785 - accuracy: 0.9523 - val_loss: 0.9035 - val_accuracy: 0.8180\n",
      "Epoch 17/20\n",
      "16/16 [==============================] - 1s 36ms/step - loss: 0.1627 - accuracy: 0.9550 - val_loss: 0.9372 - val_accuracy: 0.8160\n",
      "Epoch 18/20\n",
      "16/16 [==============================] - 0s 32ms/step - loss: 0.1532 - accuracy: 0.9568 - val_loss: 0.9270 - val_accuracy: 0.8130\n",
      "Epoch 19/20\n",
      "16/16 [==============================] - 1s 33ms/step - loss: 0.1437 - accuracy: 0.9563 - val_loss: 0.9915 - val_accuracy: 0.8030\n",
      "Epoch 20/20\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.1408 - accuracy: 0.9546 - val_loss: 0.9412 - val_accuracy: 0.8170\n"
     ]
    }
   ],
   "source": [
    "# setup logging scheme for model1\n",
    "MODEL = \"mod1-reuters-{}\".format(int(time.time()))\n",
    "tbcallback = TensorBoard(log_dir = \"logs/{}\".format(MODEL), histogram_freq=1)\n",
    "#tbcallback = TensorBoard(log_dir = \"logs\\{}\".format(MODEL), histogram_freq=1)\n",
    "\n",
    "history1 = model1.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[tbcallback])\n",
    "\n",
    "MODEL = \"mod2-reuters-{}\".format(int(time.time()))\n",
    "tbcallback = TensorBoard(log_dir = \"logs/{}\".format(MODEL), histogram_freq=1)\n",
    "#tbcallback = TensorBoard(log_dir = \"logs\\{}\".format(MODEL), histogram_freq=1)\n",
    "\n",
    "history2 = model2.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks=[tbcallback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP and return to slide to discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Retraining a model from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "18/18 [==============================] - 2s 42ms/step - loss: 2.2777 - accuracy: 0.5428\n",
      "Epoch 2/9\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 1.3639 - accuracy: 0.7043\n",
      "Epoch 3/9\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 1.0436 - accuracy: 0.7823\n",
      "Epoch 4/9\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.8299 - accuracy: 0.8327\n",
      "Epoch 5/9\n",
      "18/18 [==============================] - 0s 23ms/step - loss: 0.6696 - accuracy: 0.8686\n",
      "Epoch 6/9\n",
      "18/18 [==============================] - 1s 31ms/step - loss: 0.5414 - accuracy: 0.8942\n",
      "Epoch 7/9\n",
      "18/18 [==============================] - 1s 29ms/step - loss: 0.4446 - accuracy: 0.9135\n",
      "Epoch 8/9\n",
      "18/18 [==============================] - 1s 30ms/step - loss: 0.3677 - accuracy: 0.9290\n",
      "Epoch 9/9\n",
      "18/18 [==============================] - 0s 28ms/step - loss: 0.3066 - accuracy: 0.9375\n",
      "71/71 [==============================] - 0s 4ms/step - loss: 0.8688 - accuracy: 0.8028\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"final-reuters-{}\".format(int(time.time()))\n",
    "tbcallback = TensorBoard(log_dir = \"logs/{}\".format(MODEL), histogram_freq=1)\n",
    "#tbcallback = TensorBoard(log_dir = \"logs\\{}\".format(MODEL), histogram_freq=1)\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Dense(64, activation=\"tanh\"),\n",
    "  layers.Dense(64, activation=\"tanh\"),\n",
    "  layers.Dense(46, activation=\"softmax\")\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=9,\n",
    "          batch_size=512,\n",
    "          callbacks=[tbcallback])\n",
    "          \n",
    "results = model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9200968742370605, 0.800979495048523]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17764915405164738"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "test_labels_copy = copy.copy(test_labels)\n",
    "np.random.shuffle(test_labels_copy)\n",
    "hits_array = np.array(test_labels) == np.array(test_labels_copy)\n",
    "hits_array.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Generating predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter04_getting-started-with-neural-networks.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
